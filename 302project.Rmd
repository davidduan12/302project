---
title: "Linear Regression Analysis on Second-Handed Car Price"
author: "Aiwei Yin, Kaiyao Duan, Zhanyan Guo, Alexander Lee"
date: "2024-06-07"
output:
  pdf_document: default
  html_document: default
  heaer-includes: \usepackage{booktabs}
  word_document: default
---

# Introduction
The used car market is an ever-expanding market as the number of individuals who are interested in these vehicles has increased. Several factors have influenced the prices of used cars, and understanding these factors is essential for anyone interested in this market. Whether you are a buyer or a seller, proper analysis is required to make informed decisions in this growing market. 
“CITE literature” 
In this report, we use a linear regression model to analyze collected data and provide stakeholders with a reliable measure for predicting the expected price of used cars. By going in-depth with the intricacies of the model and its findings, buyers, and sellers will gain a better understanding of the factors driving price variations, allowing them to make educated decisions.
The response variable $(y_i)$ is the Price of Used Cars, which is a continuous random variable that follows an approximately normal distribution. The predictor variables $(x_i)$ include a combination of the following numerical variables:
Year
Kilometers Driven
Owner Type
Mileage
Engine
Power
Seats
And two categorical variables:
Fuel Type
Transmission
A linear regression model can accommodate both types of variables. We performed an analysis on the data and discovered that there isn’t much skewness in either the response or predictor variables. Furthermore, the normality and linearity assumptions required for a linear regression model are met. Based on the fitted model, the corresponding coefficient shows how changes in a single predictor, while keeping all others constant, affect the price of used cars. Thus, using a linear regression model to forecast the expected price of a used car is a good measure for predicting prices. “CITE LITERATURE”
 


```{r echo=FALSE, eval=TRUE,results='hide',fig.keep='none', message=FALSE, warning=FALSE}
# Load knitr library
library(knitr)
library(MASS)
library(Metrics)
library(car)
library(corrplot)
library(stats)

# load dataset
raw_dataset <- read.csv("data/cars.csv")
print(head(raw_dataset))
```

```{r echo=FALSE, eval=TRUE,results='hide',fig.keep='none', message=FALSE, warning=FALSE}
# shuffle the dataset
set.seed(1145)
dataset <- raw_dataset[sample(1:nrow(raw_dataset)),]
print(head(dataset))
```

# Method

We will analyze on the categorical variable Fuel Type, Transmission; and continuous variable Year, Kilometers Driven, Owner Type, Mileage, Engine, Poser, and Seats. We choose to discard Brand and Model because they contain too many categories, and would split the data set into too many categories with small number of observations in each category. For all categorical columns, we first change the ones with multiple categories into multiple single column columns, then treat the variables as dummy variables.

```{r echo=FALSE, eval=TRUE, fig.keep='none', results='hide', message=FALSE, warning=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
df <- data.frame(
  Columns = c("Fuel Type", "Transmission","Owner Type First" , "Owner Type Second", "Owner Type Third","Seat 4", "Seat 5", "Seat 7"),
  Value_1 = c("Petrol", "Automatic","First",  "Second", "Third", "4", "5", "7"),
  Value_0 = c("Diesel", "Manual",'Otherwise', "Otherwise", "Oherwise", "Otherwise","Otherwise", "Otherwise")
)

# Create table
kable(df, caption = "Treatments of categorical variables", col.names = c("Variables", "1", "0"))
```
```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
dataset$Fuel_TypePetrol <- as.numeric(dataset$Fuel_Type == 'Petrol')
dataset$TransmissionAutomatic <- as.numeric(dataset$Transmission == 'Automatic')
dataset$Owner_TypeFirst <- as.numeric(dataset$Owner_Type == 'First')
dataset$Owner_TypeSecond <- as.numeric(dataset$Owner_Type == 'Second')
dataset$Owner_TypeThird <- as.numeric(dataset$Owner_Type == 'Third')
dataset$Seats4 <- as.numeric(dataset$Seats == 4)
dataset$Seats5 <- as.numeric(dataset$Seats == 5)
dataset$Seats7 <- as.numeric(dataset$Seats == 7)
```
For seats, we believe that treating it as a categorical is better because the targeting customers of 4-seated, 5-seated, and 7-seated might be different. After experiment with all other factors being identical, we find that using seats as categorical variable increases the $R^2$ score.

For other columns, they are treated as-is.

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
# drop Brand, Model column as they are not needed
dataset <- dataset[!names(dataset) %in% c("Brand", "Model")]

# convert Fuel_Type, Transmission, Owner_Type into categorical features
dataset$Fuel_Type <- factor(dataset$Fuel_Type)
dataset$Transmission <- factor(dataset$Transmission)

dataset$Owner_Type<- factor(dataset$Owner_Type)

dataset$Seats <- factor(dataset$Seats)
head(dataset)
```

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
# split train, test dataset
set.seed(1147)
sample<-sample(c(rep(0, 0.7 * nrow(dataset)), rep(1, 0.3 * nrow(dataset))))
train_set <- dataset[sample == 0, ]
test_set <- dataset[sample == 1, ]
print(nrow(train_set))
print(nrow(test_set))
head(train_set)
head(test_set)
```

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
summary(train_set)
summary(test_set)
```

## Data inspection

### Continuous Variables
```{r echo=FALSE, eval=TRUE, results=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(reshape)
train_set_plot <- data.frame(train_set$Year,train_set$Kilometers_Driven, train_set$Mileage, train_set$Engine, train_set$Power)
meltData <- melt(train_set_plot)
p <- ggplot(meltData, aes(factor(variable), value)) 
p + geom_boxplot() + facet_wrap(~variable, scale="free")
```

A few outliers appear in Power, Kilometers_driven and Milage, we will eliminate them using Cook's distance

### Categorical Variables

```{r echo=FALSE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
par(mfrow=c(2,4))
plot(train_set$Fuel_TypePetrol, train_set$Price,
      xlab = "Fuel Type = Petrol", ylab = "Price", main = "Fuel Type vs Price")
#axis(1, labels = levels(train_set$Fuel_Type))

plot(train_set$TransmissionAutomatic, train_set$Price,
      xlab = "Transmission = Automatic", ylab = "Price", main = "Transmission vs Price")

plot(train_set$Owner_TypeFirst, train_set$Price,
      xlab= "Owner Type = First Handed", ylab="Price", main = "Owner Type vs Price")
#axis(1,  labels = levels(train_set$Transmission))

plot(train_set$Owner_TypeSecond, train_set$Price,
      xlab = "Owner Type = Second Handed", ylab = "Price", main = "Owner Type vs Price")
#axis(1, labels = levels(train_set$Owner_Type))

plot(train_set$Owner_TypeThird, train_set$Price,
      xlab = "Owner Type = Third Handed", ylab = "Price", main = "Owner Type vs Price")
#axis(1, labels = levels(train_set$Owner_Type))

plot(train_set$Seats4, train_set$Price, 
     xlab="Seats = 4", ylab="Price", main = "Seats vs Price")

plot(train_set$Seats5, train_set$Price, 
     xlab = "Seats = 5", ylab = "Price", main = "Seats vs Price")

plot(train_set$Seats7, train_set$Price, 
     xlab = "Seats = 7", ylab = "Price", main = "Seats vs Price")

# Reset the plotting layout
par(mfrow = c(1, 1))
```

We find that ownly Transmission and First handed have a significant difference in data, while Seats do not have enough observations, and Fuel type 


## Models

### Simple Linear Model

```{r echo = FALSE, eval = TRUE, results="hide", fig.keep = "none", message=FALSE, warning=FALSE}
simple_model <- lm(Price ~ Year + Kilometers_Driven + Fuel_Type + Transmission + Owner_TypeFirst + Mileage + Engine + Power, data=train_set)

summary(simple_model)
plot(simple_model)
```

graph to include in report

```{r echo=FALSE, results='hide', message=FALSE, warning=FALSE}
plot(simple_model, which=1, main="Residual vs Fitted")
plot(simple_model, which=2, main="Normal QQ plot")
```

### Outliers

We will use cook's distance to remove outliers, we choose $3/n$ as the threashold of outliers.

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
cook <- cooks.distance(simple_model)
reduced_train_set <- train_set[cook < 3/nrow(train_set),]
```

### box-cox
We can see that linearity is violated since the fit line (red) is quite far from flat, and constant variance, and uncorrelated error are also violated because at small values the residuals are much smaller. We will use **box cox tranformation method**

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
b <- boxcox(simple_model)
lambda <- b$x[which.max(b$y)]

```

we find that the optimal lambda is $\lambda = `r lambda`$


```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
# calculate transformed
reduced_train_set$Price_transformed <- (reduced_train_set$Price^lambda - 1)/lambda
test_set$Price_transformed <- (test_set$Price^lambda - 1)/lambda
```

### VIF
Then we will use VIF to determine the colinearity of columns

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
vif_scores<-vif(simple_model)
correlations <- round(cor(reduced_train_set[sapply(reduced_train_set, is.numeric)]),2)
```

```{r echo=FALSE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
vif_df <- data.frame(vif_scores)
kable(vif_df, caption = "Variance Inflation Factors")
```

Year and Kilometer have a very high VIF, we choose to discard them as their. However for Power and Engine, we suspect the high VIF come from the fact that these two variables are highly correlated.


```{r echo=FALSE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
corrplot(correlations, method="circle", type="upper")
engine_power_cov <- data.frame(correlations)$Power[5]
engine_price_cov <- data.frame(correlations)$Price_transformed[5]
price_power_cov <- data.frame(correlations)$Price_transformed[6]
```
The covariance $Cov(Power, Engine) = `r engine_power_cov`$ shows a high colinearity, we choose to discard Engine, which has lower covariance with Price ($Cov(Engine, Price\_transformed) = `r engine_price_cov`$, whereas $Cov(Power, Price_transformed) = `r price_power_cov`$ )


### Model of choice

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
better_transformed_model <- lm(Price_transformed ~ Fuel_Type + Transmission + Owner_TypeFirst + Mileage + Power, data = reduced_train_set)
plot(better_transformed_model)
summary(better_transformed_model)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
plot(better_transformed_model, which=1, main="Residual vs Fitted")
plot(better_transformed_model, which=2, main="Normal QQ plot")
```

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
predictions <- predict(better_transformed_model, newdata = test_set)
```


```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
test_model = function(model, column) {
  predictions = predict(model, newdata = test_set)

  # Calculate residuals
  residuals = column - predictions
  
  # Calculate Mean Absolute Error
  mae_value = mae(column, predictions)
  
  # Calculate Mean Squared Error
  mse_value = mse(column, predictions)
  
  # Calculate R-squared
  rsq_value = cor(column, predictions)^2
  
  # Calculate standard error of residuals
  std_error_residuals = sqrt(sum(residuals^2) / (length(residuals) - 2))
  
  
  cat("\nResidual standard error:", std_error_residuals, "\n")
  cat("Multiple R-squared:", rsq_value, "\n")
  cat("Mean Absolute Error (MAE):", mae_value, "\n")
  cat("Mean Squared Error (MSE):", mse_value, "\n")
    
}
```

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
test_model(better_transformed_model, test_set$Price_transformed)
```
