---
title: "Linear Regression Analysis on Second-Handed Car Price"
author: "Aiwei Yin, Kaiyao Duan, Zhanyan Guo, Alexander Lee"
date: "2024-06-07"
output:
  pdf_document: default
  html_document: default
  heaer-includes: \usepackage{booktabs}
  word_document: default
---

# Introduction
The demand for cars in modern society has grown significantly, leading to a robust market for second-hand vehicles. Unlike the straightforward process of purchasing a new car from a dealership with fixed prices, buying a used car involves numerous factors that can complicate the decision-making process. Understanding these factors is crucial for consumers to make informed choices in the second-hand car market.

Previous research has extensively explored the determinants of second-hand car prices using various methods. (Pudaruth 2014) conducted a comparative study on multiple linear regression, k-nearest-neighbors (kNN), decision trees, and Naive Bayes to predict used car prices. The study found that while linear regression did not perform as well as kNN, the year of manufacture was a more significant predictor of car prices than mileage. Additionally, it was noted that logarithmic regression slightly outperformed linear regression in predictive accuracy. However, due to missing data, the application of linear regression was limited to using only the year and mileage as predictor variables.

Another study highlighted the critical role of data pre-processing in improving predictive models. By removing outliers, noisy values, and irrelevant columns, the coefficient of determination (RÂ²) improved from 0.62 to 0.73. This improvement underscores the importance of data quality and pre-processing techniques in enhancing the accuracy of predictive models for used car prices.(Muti & Yildiz 2023)


```{r echo=FALSE, eval=TRUE,results='hide',fig.keep='none', message=FALSE, warning=FALSE}
# Load knitr library
library(knitr)
library(MASS)
library(Metrics)
library(car)
library(corrplot)
library(stats)

# load dataset
raw_dataset <- read.csv("data/cars.csv")
print(head(raw_dataset))
```

```{r echo=FALSE, eval=TRUE,results='hide',fig.keep='none', message=FALSE, warning=FALSE}
# shuffle the dataset
set.seed(1145)
dataset <- raw_dataset[sample(1:nrow(raw_dataset)),]
print(head(dataset))
```

# Method
This study finds the optimal linear regression model to predict the Price of second handed cars given the multiple predictor variables: Brand, Model, Year, Kilometers Driven, Fuel Type, Transmission Type, Owner Type, Mileage, Engine, Power, and Number of Seats.

We begin with preprocessing the data set, turning all the columns with text values that represent categorical variables into dummy variables. Then we will randomly shuffle the data, and partition it into 70% of training set and 30% testing set.

For all categorical columns, we first change the ones with multiple categories into multiple single column columns, then treat the variables as dummy variables. For categorical variables that have multiple categories, we create a dummy variable for each of the categories.

To find out the relation between each of the variables and the our response variable, we first analyze the scatter plot between each of the predictor variables and Price, and discard both the continuous variables that appears completely random and categorical variables that partition the data set into too many small chunks. Then we create a simple multilinear model without any modifications, to provide a baseline model for us to asses the outcomes of our methods. We first check violations of assumptions (linearity, uncorrelated errors, constant variance) with Normal Quantile-Quantile (Q-Q) plots and Residual vs Fitted Values plot. Ideally, the normal Q-Q plot should appear to be a straight diagonal line, and the Residual vs Predict value plot should look completely randomly distributed.

Based on the simple multilinear model, we calculate the Cook's Distance of all the observations in order to find the outliers. We choose to remove all the observations with Cook's distant $\geq 3/n$.

To resolve the violation of the above assumptions, we will use three methods. We will first use transformation on the response variable, choosing the optimal between box-cox transformation, log transformation and power transformation based on the normal QQ plot and residual plots.

After the transformations we will select the predictor variables based on the multi-colinearity with the VIF scores based on the privious simple multilinear model. We choose to analyze the variables with VIF $>0.3$. There are two possible situations, one is that there is no significant linear relationship with price, and one is that there is a high correlation between two of the predictor variables. For the first situation, we choose to directly discard the predictor variable. For the second situation, we choose to discard the predictor variable with lower correlation with Price, and keep the predictor variable with higher covariance.

When we select the preferred model, we will again assess the assumptions above, and validate our model by examining the models with Analysis of Variance (ANOVA) tests, if the P-tests for all the predictor variable show a P-value of less than 0.05, we consider the model as significant. Furthermore we will compare the $R^2$ value of both models, if the $R^2$ value is indeed higher we choose to use the model we selected.

# Results

```{r echo=FALSE, eval=TRUE, fig.keep='none', results='hide', message=FALSE, warning=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
df <- data.frame(
  Columns = c("Fuel Type", "Transmission","Owner Type First" , "Owner Type Second", "Owner Type Third","Seat 4", "Seat 5", "Seat 7"),
  Value_1 = c("Petrol", "Automatic","First",  "Second", "Third", "4", "5", "7"),
  Value_0 = c("Diesel", "Manual",'Otherwise', "Otherwise", "Oherwise", "Otherwise","Otherwise", "Otherwise")
)

# Create table
kable(df, caption = "Treatments of categorical variables", col.names = c("Variables", "1", "0"))
```
```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
dataset$Fuel_TypePetrol <- as.numeric(dataset$Fuel_Type == 'Petrol')
dataset$TransmissionAutomatic <- as.numeric(dataset$Transmission == 'Automatic')
dataset$Owner_TypeFirst <- as.numeric(dataset$Owner_Type == 'First')
dataset$Owner_TypeSecond <- as.numeric(dataset$Owner_Type == 'Second')
dataset$Owner_TypeThird <- as.numeric(dataset$Owner_Type == 'Third')
dataset$Seats4 <- as.numeric(dataset$Seats == 4)
dataset$Seats5 <- as.numeric(dataset$Seats == 5)
dataset$Seats7 <- as.numeric(dataset$Seats == 7)
```
For seats, we believe that treating it as a categorical is better because the targeting customers of 4-seated, 5-seated, and 7-seated might be different. After experiment with all other factors being identical, we find that using seats as categorical variable increases the $R^2$ score.

For other columns, they are treated as-is.

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
# drop Brand, Model column as they are not needed
dataset <- dataset[!names(dataset) %in% c("Brand", "Model")]

# convert Fuel_Type, Transmission, Owner_Type into categorical features
dataset$Fuel_Type <- factor(dataset$Fuel_Type)
dataset$Transmission <- factor(dataset$Transmission)

dataset$Owner_Type<- factor(dataset$Owner_Type)

dataset$Seats <- factor(dataset$Seats)
head(dataset)
```

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
# split train, test dataset
set.seed(1147)
sample<-sample(c(rep(0, 0.7 * nrow(dataset)), rep(1, 0.3 * nrow(dataset))))
train_set <- dataset[sample == 0, ]
test_set <- dataset[sample == 1, ]
print(nrow(train_set))
print(nrow(test_set))
head(train_set)
head(test_set)
```

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
summary(train_set)
summary(test_set)
```


### Continuous Variables
```{r echo=FALSE, eval=TRUE, results=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(reshape)
train_set_plot <- data.frame(train_set$Year,train_set$Kilometers_Driven, train_set$Mileage, train_set$Engine, train_set$Power)
meltData <- melt(train_set_plot)
p <- ggplot(meltData, aes(factor(variable), value)) 
p + geom_boxplot() + facet_wrap(~variable, scale="free")
```

From the box plots, we find there there are a few outliers that could potentially be leveraging points. We will use Cook's distance to eliminate bad leveraging points below.

### Categorical Variables

```{r echo=FALSE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
par(mfrow=c(2,4))
plot(train_set$Fuel_TypePetrol, train_set$Price,
      xlab = "Fuel Type = Petrol", ylab = "Price", main = "Fuel Type vs Price")
#axis(1, labels = levels(train_set$Fuel_Type))

plot(train_set$TransmissionAutomatic, train_set$Price,
      xlab = "Transmission = Automatic", ylab = "Price", main = "Transmission vs Price")

plot(train_set$Owner_TypeFirst, train_set$Price,
      xlab= "Owner Type = First Handed", ylab="Price", main = "Owner Type vs Price")
#axis(1,  labels = levels(train_set$Transmission))

plot(train_set$Owner_TypeSecond, train_set$Price,
      xlab = "Owner Type = Second Handed", ylab = "Price", main = "Owner Type vs Price")
#axis(1, labels = levels(train_set$Owner_Type))

plot(train_set$Owner_TypeThird, train_set$Price,
      xlab = "Owner Type = Third Handed", ylab = "Price", main = "Owner Type vs Price")
#axis(1, labels = levels(train_set$Owner_Type))

plot(train_set$Seats4, train_set$Price, 
     xlab="Seats = 4", ylab="Price", main = "Seats vs Price")

plot(train_set$Seats5, train_set$Price, 
     xlab = "Seats = 5", ylab = "Price", main = "Seats vs Price")

plot(train_set$Seats7, train_set$Price, 
     xlab = "Seats = 7", ylab = "Price", main = "Seats vs Price")

# Reset the plotting layout
par(mfrow = c(1, 1))
```

We find that ownly Transmission and First handed have a significant difference in data, while Seats do not have enough observations, and Fuel type have very similar distributions.


### Simple Linear Model

Prior to cleaning data, we will first make a linear model to provide a baseline to compare the effectiveness of our methods, and also to provide a model to perform box-cox transformation, cook's score

```{r echo = FALSE, eval = TRUE, results="hide", fig.keep = "none", message=FALSE, warning=FALSE}
simple_model <- lm(Price ~ Year + Kilometers_Driven + Fuel_Type + Transmission + Owner_TypeFirst + Mileage + Engine + Power, data=train_set)

summary(simple_model)
plot(simple_model)
```


```{r echo=FALSE, results='hide', message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
plot(simple_model, which=1, main="Residual vs Fitted")
plot(simple_model, which=2, main="Normal QQ plot")
par(mfrow=c(1,1))
```

From the Residual vs Fitted graph, we find that both linearity and

### Outliers

We will use cook's distance to remove outliers, we choose $3/n$ as the threashold of outliers.

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
cook <- cooks.distance(simple_model)
reduced_train_set <- train_set[cook < 3/nrow(train_set),]
```

```{r echo=FALSE, eval=TRUE, results="hide", message=FALSE, warning=FALSE}
train_set_plot <- data.frame(reduced_train_set$Year, reduced_train_set$Kilometers_Driven, reduced_train_set$Mileage, reduced_train_set$Engine, reduced_train_set$Power)
meltData <- melt(train_set_plot)
p <- ggplot(meltData, aes(factor(variable), value)) 
p + geom_boxplot() + facet_wrap(~variable, scale="free")
```
We can see that some outliers gets effectively removed.

### box-cox
We can see that linearity is violated since the fit line (red) is quite far from flat, and constant variance, and uncorrelated error are also violated because at small values the residuals are much smaller. We will use **box cox tranformation method**

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
b <- boxcox(simple_model)
lambda <- b$x[which.max(b$y)]
```

we find that the optimal lambda is $\lambda = `r lambda`$


```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
# calculate transformed
reduced_train_set$Price_transformed <- (reduced_train_set$Price^lambda - 1)/lambda
test_set$Price_transformed <- (test_set$Price^lambda - 1)/lambda
```

### VIF
Then we will use VIF to determine the colinearity of columns

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
vif_scores<-vif(simple_model)
correlations <- round(cor(reduced_train_set[sapply(reduced_train_set, is.numeric)]),2)
```

```{r echo=FALSE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
vif_df <- data.frame(vif_scores)
kable(vif_df, caption = "Variance Inflation Factors")
```

Year and Kilometer have a very high VIF, similar results could be shown by inspecting the plot of these two variables, we choose to discard them. However for Power and Engine, we suspect the high VIF come from the fact that these two variables are highly correlated.

```{r echo=FALSE, results='hide'}
par(mfrow = c(1,2))
plot(train_set$Kilometers_Driven, train_set$Price, main="Kilometers vs Price", xlab = "Kilometers", ylab = "Price")
plot(train_set$Year, train_set$Price, main="Year vs Price", xlab = "Year", ylab = "Price")
par(mfrow=c(1,1))
```

```{r echo=FALSE, eval=TRUE, results='hide', message=FALSE, warning=FALSE}
corrplot(correlations, method="circle", type="upper")
engine_power_cov <- data.frame(correlations)$Power[5]
engine_price_cov <- data.frame(correlations)$Price_transformed[5]
price_power_cov <- data.frame(correlations)$Price_transformed[6]
```
The covariance $Cov(Power, Engine) = `r engine_power_cov`$ shows a high colinearity, we choose to discard Engine, which has lower covariance with Price ($Cov(Engine, Price) = `r engine_price_cov`$, whereas $Cov(Power, Price) = `r price_power_cov`$ )


### Model of choice

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
better_transformed_model <- lm(Price_transformed ~ Fuel_Type + Transmission + Owner_TypeFirst + Mileage + Power, data = reduced_train_set)
plot(better_transformed_model)
summary(better_transformed_model)
```
```{r echo=FALSE, warning=FALSE, error=FALSE,}
rsq_good <- summary(better_transformed_model)$r.squared
rsq_bad <- summary(simple_model)$r.squared
```

In the transformed model, we achieved a R-squared of $`r rsq_good`$, an increase of $`r rsq_good - rsq_bad`$ compared to the baseline model ($`r rsq_bad`$). We have also 

```{r echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
plot(better_transformed_model, which=1, main="Residual vs Fitted")
plot(better_transformed_model, which=2, main="Normal QQ plot")
par(mfrow=c(1,1))
```
Rresults show that our treatment of  removing outliers with cook's distance, using box-cox transformation, and utilizing VIF to remove useless variables improves our model effectively.

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
predictions <- predict(better_transformed_model, newdata = test_set)
```


```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
test_model = function(model, column) {
  predictions = predict(model, newdata = test_set)

  # Calculate residuals
  residuals = column - predictions
  
  # Calculate Mean Absolute Error
  mae_value = mae(column, predictions)
  
  # Calculate Mean Squared Error
  mse_value = mse(column, predictions)
  
  # Calculate R-squared
  rsq_value = cor(column, predictions)^2
  
  # Calculate standard error of residuals
  std_error_residuals = sqrt(sum(residuals^2) / (length(residuals) - 2))
  
  
  cat("\nResidual standard error:", std_error_residuals, "\n")
  cat("Multiple R-squared:", rsq_value, "\n")
  cat("Mean Absolute Error (MAE):", mae_value, "\n")
  cat("Mean Squared Error (MSE):", mse_value, "\n")
    
}
```

```{r echo=FALSE, eval=TRUE, results='hide', fig.keep='none', message=FALSE, warning=FALSE}
test_model(better_transformed_model, test_set$Price_transformed)
```
